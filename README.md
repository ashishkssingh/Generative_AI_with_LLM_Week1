# Generative AI with LLM: Week 1
Generative AI with Large Language Models course offered by Coursera in partnership with DeepLearning.AI

## Objective

This week dive into pre-training strategies for large language models (LLMs), exploring their benefits and comparing them to fine-tuning. You'll grasp key LLM concepts and delve into the model lifecycle, learning to navigate limitations and optimize processes for efficiency and scalability.

## Topics Learned

- Generative AI project lifecycle
	- What are Generative AI and LLMs
	- LLM use cases and tasks
	- Text generation before transformers using RNNs
	- Transformation architecture
	- How Transformers generates texts
	- Transformers: Attention is All You Need (Amazing read) - [Attention Is All You Need (arxiv.org)](https://arxiv.org/abs/1706.03762)

- LLM Pre-training and scaling laws
	- What is Prompting and prompt Engineering
	- How generative configuration can provide different results
	- What does the Initial training process for LLMs look like a.k.a pre-training phase
	- What strategies can be applied using Efficient multi-GPU computing?
	- What are the scaling laws and how to compute-optimal models
	- How Bloomberg used Chinchilla scaling laws to find the perfect balance between a number of parameters in the model and the volume of training data. 
	- Another great read if you in the business of optimizing LLMs for your use-case that illustrates pre-training of model for increased domain-specificity, and the challenges that may force trade-offs against compute-optimal model and training configurations - [BloombergGPT: A Large Language Model for Finance (arxiv.org)](https://arxiv.org/abs/2303.17564)
